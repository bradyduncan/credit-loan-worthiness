{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4016d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994ea7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33682706",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd439f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "data_dir = os.path.join(os.getcwd(), 'home-credit-default-risk')\n",
    "test_path = os.path.join(data_dir, 'application_test.csv')\n",
    "train_path = os.path.join(data_dir, 'application_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096f06a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load DataFrames\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5bda7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a8607c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
       "       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
       "       'AMT_CREDIT', 'AMT_ANNUITY',\n",
       "       ...\n",
       "       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
       "       'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
       "       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       "       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
       "       'AMT_REQ_CREDIT_BUREAU_YEAR'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1a4fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08072881945686496"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['TARGET'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b8890",
   "metadata": {},
   "source": [
    "A `1` in the `TARGET` column indicates that the client defaulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53a0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace {Y, N} with {1, 0}\n",
    "for column in df_train.columns:\n",
    "    if df_train[column].dtype == 'object':\n",
    "        df_train[column] = df_train[column].replace({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7816cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split numerical and categorical data\n",
    "numerical_cols = df_train.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df_train.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# impute missing values based on mean\n",
    "num_imp = SimpleImputer(strategy='mean')\n",
    "num_data_imp = num_imp.fit_transform(df_train[numerical_cols])\n",
    "num_df = pd.DataFrame(num_data_imp, columns=numerical_cols)\n",
    "\n",
    "# impute missing values based on most frequent\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "cat_data_imp = cat_imp.fit_transform(df_train[categorical_cols])\n",
    "cat_df = pd.DataFrame(cat_data_imp, columns=categorical_cols)\n",
    "\n",
    "# concatenate back into complete dataframe\n",
    "df_train_clean = pd.concat([num_df, cat_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be7381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brady\\AppData\\Local\\Temp\\ipykernel_24648\\3789349187.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  abs_corrs = df_train_clean.corr()['TARGET'].abs().sort_values(ascending=False)[1:11]\n",
      "C:\\Users\\brady\\AppData\\Local\\Temp\\ipykernel_24648\\3789349187.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  top_corrs = df_train_clean.corr()['TARGET'].loc[abs_corrs.index]\n"
     ]
    }
   ],
   "source": [
    "# choose the top 10 highest correlation columns (positive or negative)\n",
    "abs_corrs = df_train_clean.corr()['TARGET'].abs().sort_values(ascending=False)[1:11]\n",
    "top_corrs = df_train_clean.corr()['TARGET'].loc[abs_corrs.index]\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_corrs.index, y=top_corrs.values, palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Highest Correlations to Loan Defaulting\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Correlation with Target\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d20f76",
   "metadata": {},
   "source": [
    "`EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3` are credit scores from external sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc212a0-36ce-4e13-986f-cf1488a485d6",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0df51-e845-4d3b-974f-4c60c8dbf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df_train_clean.drop('TARGET', axis=1)  # Drop the target column to separate features\n",
    "y = df_train_clean['TARGET']  # Separate the target\n",
    "\n",
    "# Splitting the dataset into training and testing parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features (optional based on model requirements)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Combining the resampled data back into a new dataframe\n",
    "import pandas as pd\n",
    "df_train_smote = pd.DataFrame(X_train_smote, columns=X_train.columns)\n",
    "df_train_smote['TARGET'] = y_train_smote  # Adding the target column back\n",
    "\n",
    "df_test_clean = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "df_test_clean['TARGET'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d85f5",
   "metadata": {},
   "source": [
    "## Find the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the columns that have the strongest correlations\n",
    "cols = list(top_corrs.index)\n",
    "\n",
    "# filter out only the strongest correlation columns and target\n",
    "X = df_train_clean[cols]\n",
    "y = df_train_clean['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation set to find best performing model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc5f86",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb150c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# find training and validation accuracy scores\n",
    "print(f'Training Accuracy: {rf_clf.score(X_train, y_train):.4f}')\n",
    "print(f'Test Accuracy: {rf_clf.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y prediction values\n",
    "y_pred_rf=(rf_clf.predict(X_test))\n",
    "\n",
    "# create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221bed7a",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85442072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit GB classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# find training and validation accuracy scores\n",
    "print(f'Training Accuracy: {gb_clf.score(X_train, y_train):.4f}')\n",
    "print(f'Test Accuracy: {gb_clf.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca382c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y prediction values\n",
    "y_pred_gb=(gb_clf.predict(X_test))\n",
    "\n",
    "# create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_gb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix - Gradient Boosting Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bd680",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit XGB classifier\n",
    "xgb_clf = XGBClassifier(random_state=42, max_depth=3, learning_rate=.1)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# find training and validation accuracy scores\n",
    "print(f'Training Accuracy: {xgb_clf.score(X_train, y_train):.4f}')\n",
    "print(f'Test Accuracy: {xgb_clf.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y prediction values\n",
    "y_pred_xgb=(xgb_clf.predict(X_test))\n",
    "\n",
    "# create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix - Extreme Gradient Boosting Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af608c1e-f132-46fa-ac1c-9ebf9f6657d0",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87162a-a7aa-4ede-8ea5-faf12215666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = df_train_clean[cols]  \n",
    "y = df_train_clean['TARGET']  \n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MLP Classifier setup\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50, 30), activation='relu', solver='adam', max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions and evaluating the model\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79282ca3-7a61-48da-9bd2-e71a72878174",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a1a90-fc4e-407e-82d2-d331698e5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df_train_clean[cols]\n",
    "y = df_train_clean['TARGET']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=300, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10957c74",
   "metadata": {},
   "source": [
    "## Choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59510cb2",
   "metadata": {},
   "source": [
    "Based on the extremely similar scores coming from each of the Ensemble Methods models, we decided to choose the Random Forest Classifier as the model that would best predict whether a client would default or not. This is because out of all of the classifiers, it had the fewest number of false negatives (Predicted the client would not default, but they did). We chose this due to the nature of the problem. Predicting a client is reliable and approving them when they actually aren't has far worse implications than falsely rejecting an application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e8d7a",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9af8cf",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32daf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace {Y, N} with {1, 0}\n",
    "for column in df_test.columns:\n",
    "    if df_test[column].dtype == 'object':\n",
    "        df_test[column] = df_test[column].replace({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308aefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split numerical and categorical data\n",
    "numerical_cols = df_test.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df_test.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# impute missing values based on mean\n",
    "num_imp = SimpleImputer(strategy='mean')\n",
    "num_data_imp = num_imp.fit_transform(df_test[numerical_cols])\n",
    "num_df = pd.DataFrame(num_data_imp, columns=numerical_cols)\n",
    "\n",
    "# impute missing values based on most frequent\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "cat_data_imp = cat_imp.fit_transform(df_test[categorical_cols])\n",
    "cat_df = pd.DataFrame(cat_data_imp, columns=categorical_cols)\n",
    "\n",
    "# concatenate back into complete dataframe\n",
    "df_test_clean = pd.concat([num_df, cat_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train new classifier on the entire training dataset\n",
    "test_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "test_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b74c1a",
   "metadata": {},
   "source": [
    "`X` and `y` are the cleaned training dataframe filtered for only the strongest correlation columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4030655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out only the strongest correlation columns\n",
    "X_test_pred = df_test_clean[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test data\n",
    "y_pred_test=(test_clf.predict(X_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c4785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find the 1 predicted values\n",
    "y_pred_true = y_pred_test[y_pred_test == 1]\n",
    "\n",
    "# find the percentage of predicted 1s\n",
    "pred_percentage = (len(y_pred_true) / len(y_pred_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822aff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 1 true values\n",
    "y_true = y[y == 1]\n",
    "\n",
    "# find the percentage of true 1s\n",
    "true_percentage = (len(y_true) / len(y)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Actual Percentage: {true_percentage}')\n",
    "print(f'Predicted Percentage: {pred_percentage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82142716",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Actual Default Percentage', 'Predicted Default Percentage']\n",
    "values = [true_percentage, pred_percentage]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "colors = ['slateblue', 'darkblue']\n",
    "\n",
    "# plot first subplot with percentage range 0-100\n",
    "axs[0].bar(labels, values, color=colors)\n",
    "for bar, value in zip(axs[0].patches, values):\n",
    "    axs[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}%', ha='center', va='bottom')\n",
    "axs[0].set_ylabel('Percentage')\n",
    "axs[0].set_title('Comparison of True and Predicted Percentages of Positive Labels (1s)')\n",
    "axs[0].set_ylim(0, 100)\n",
    "\n",
    "# plot second  subplot with percentage range 0-10\n",
    "axs[1].bar(labels, values, color=colors)\n",
    "for bar, value in zip(axs[1].patches, values):\n",
    "    axs[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}%', ha='center', va='bottom')\n",
    "axs[1].set_ylabel('Percentage')\n",
    "axs[0].set_title('Comparison of True and Predicted Percentage of Default Labels')\n",
    "axs[1].set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642cb4e",
   "metadata": {},
   "source": [
    "This is a very interesting relationship. It appears the models are trained to prioritize avoiding false negatives over false positives for the sake of accuracy. However, in this situation, it would be beneficial to predict a higher number of true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c59afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision (training data, test set): {precision_score(y_test, y_pred_rf)}')\n",
    "print(f'Recall (training data, test set): {recall_score(y_test, y_pred_rf)}')\n",
    "print(f'F1 Score (training data, test set): {f1_score(y_test, y_pred_rf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fb964",
   "metadata": {},
   "source": [
    "These suspicions were confirmed with the shockingly low precision, recall, and F1 scores of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46100fa5",
   "metadata": {},
   "source": [
    "### Threshold Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b123b9",
   "metadata": {},
   "source": [
    "In order to avoid a high accuracy, low effectiveness model, we next experimented with thresholds. We tested each threshold from .05 to .4 and found which one would return the least false positives while also maintaining high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "thresholds = np.arange(0.05, 0.45, 0.05)\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "tps = []\n",
    "fps = []\n",
    "tns = []\n",
    "fns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each threshold\n",
    "for threshold in thresholds:\n",
    "    # initialize and train the RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # predict probabilities and compare to threshold\n",
    "    y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "    y_pred_adjusted = (y_pred_prob > threshold).astype(int)\n",
    "    \n",
    "    # calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_adjusted)\n",
    "    confusion_matrices.append(conf_matrix)\n",
    "    \n",
    "    # fetch all values from confusion matrix\n",
    "    tp = conf_matrix[1, 1]\n",
    "    fp = conf_matrix[0, 1]\n",
    "    tn = conf_matrix[0, 0]\n",
    "    fn = conf_matrix[1, 0]\n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)\n",
    "    \n",
    "    # calculate and append scores\n",
    "    acc = accuracy_score(y_test, y_pred_adjusted)\n",
    "    accuracies.append(acc)\n",
    "    rec = recall_score(y_test, y_pred_adjusted)\n",
    "    recalls.append(rec)\n",
    "    prec = precision_score(y_test, y_pred_adjusted)\n",
    "    precisions.append(prec)\n",
    "    f1 = f1_score(y_test, y_pred_adjusted)\n",
    "    f1s.append(f1)\n",
    "\n",
    "# store results in DF\n",
    "results_df = pd.DataFrame({\n",
    "    'threshold': thresholds,\n",
    "    'confusion_matrix': confusion_matrices,\n",
    "    'accuracy': accuracies,\n",
    "    'recall': recalls,\n",
    "    'precision': precisions,\n",
    "    'F1': f1s,\n",
    "    'TP': tps,\n",
    "    'FP': fps,\n",
    "    'TN': tns,\n",
    "    'FN': fns\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots for each threshold\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    cm = confusion_matrices[i]\n",
    "    axs[row, col].imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    axs[row, col].set_title(f'Threshold: {threshold}:')\n",
    "    axs[row, col].set_xlabel('Predicted labels')\n",
    "    axs[row, col].set_ylabel('True labels')\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            axs[row, col].text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b8e41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70f8dd",
   "metadata": {},
   "source": [
    "Based on the data, we chose .1 as a reasonable decision threshold that increases recall and f1 without completely tanking accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe39019",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_1 = results_df['confusion_matrix'][1]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_1, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix - Random Forest Classifier (Decision Threshold .1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the new decision threshold\n",
    "threshold = .1\n",
    "test_pred_prob = test_clf.predict_proba(X_test_pred)[:, 1]\n",
    "y_pred_test = (test_pred_prob > .1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72687a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 1 predicted values\n",
    "y_pred_true = y_pred_test[y_pred_test == 1]\n",
    "\n",
    "# find the percentage of predicted 1s\n",
    "pred_percentage = (len(y_pred_true) / len(y_pred_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a3433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Actual Percentage: {true_percentage}')\n",
    "print(f'Predicted Percentage: {pred_percentage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Actual Default Percentage', 'Predicted Default Percentage']\n",
    "values = [true_percentage, pred_percentage]\n",
    "\n",
    "# plot bars and add labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(labels, values, color=colors)\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Comparison of True and Predicted Percentage of Default Labels (Decision Threshold .1)')\n",
    "plt.ylim(0, 50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a6359",
   "metadata": {},
   "source": [
    "While the rate of positive (default) classifications is now much higher than the actual default percentage, this is preferred because we would much rather have the program incorrectly classify an applicant as high risk than mistakenly let a risky applicant through who later defaults on the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9aa19",
   "metadata": {},
   "source": [
    "The model does not currently work well. An increase in correlated features may improve it in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
